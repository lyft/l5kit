{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision.models.resnet import resnet50\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.dataset.utilities import build_dataloader\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_coords_as_csv, compute_mse_error_csv\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data path and load cfg\n",
    "\n",
    "By setting the `L5KIT_DATA_FOLDER` variable, we can point the script to the folder where the data lies.\n",
    "\n",
    "Then, we load our config file with relative paths and other configurations (rasterer, training params...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('format_version', 4), ('model_params', OrderedDict([('model_architecture', 'resnet50'), ('history_num_frames', 0), ('history_step_size', 1), ('history_delta_time', 0.1), ('future_num_frames', 50), ('future_step_size', 1), ('future_delta_time', 0.1)])), ('raster_params', OrderedDict([('raster_size', [224, 224]), ('pixel_size', [0.5, 0.5]), ('ego_center', [0.25, 0.5]), ('map_type', 'py_semantic'), ('satellite_map_key', 'aerial_map/aerial_map.png'), ('semantic_map_key', 'semantic_map/semantic_map.pb'), ('filter_agents_threshold', 0.5)])), ('train_data_loader', OrderedDict([('datasets', [OrderedDict([('key', 'sample_scenes/sample.zarr'), ('scene_indices', [-1])])]), ('perturb_probability', 0.0), ('batch_size', 12), ('shuffle', True), ('num_workers', 16)])), ('val_data_loader', OrderedDict([('datasets', [OrderedDict([('key', 'sample_scenes/sample.zarr'), ('scene_indices', [0])])]), ('perturb_probability', 0.0), ('batch_size', 12), ('shuffle', False), ('num_workers', 16)])), ('train_params', OrderedDict([('checkpoint_every_n_steps', 10000), ('max_num_steps', 5), ('eval_every_n_steps', 10000)]))])\n"
     ]
    }
   ],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/Users/sspeichert/Documents/LyftData/\"\n",
    "# get config\n",
    "cfg = load_config_data(\"./agent_motion_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Our baseline is a simple `resnet50` pretrained on `imagenet`. We must replace the input and the final layer to address our requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg: Dict) -> torch.nn.Module:\n",
    "    # load pre-trained Conv2D model\n",
    "    model = resnet50(pretrained=True)\n",
    "\n",
    "    # change input size\n",
    "    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "    num_in_channels = 3 + num_history_channels\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        num_in_channels,\n",
    "        model.conv1.out_channels,\n",
    "        kernel_size=model.conv1.kernel_size,\n",
    "        stride=model.conv1.stride,\n",
    "        padding=model.conv1.padding,\n",
    "        bias=False,\n",
    "    )\n",
    "    # change output size\n",
    "    # X, Y  * number of future states\n",
    "    num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "    model.fc = nn.Linear(in_features=2048, out_features=num_targets)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data, model, device, criterion):\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    targets = data[\"target_positions\"].to(device).reshape(len(data[\"target_positions\"]), -1)\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss = loss.mean()\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and initialise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = LocalDataManager(None)\n",
    "# ===== INIT DATASETS\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_dataloader = build_dataloader(cfg, \"train\", dm, AgentDataset, rasterizer)\n",
    "eval_dataloader = build_dataloader(cfg, \"val\", dm, AgentDataset, rasterizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== INIT MODEL\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_model(cfg).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 261.3131103515625 loss(avg): 377.76092529296875: 100%|██████████| 5/5 [00:39<00:00,  7.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# ==== TRAIN LOOP\n",
    "tr_it = iter(train_dataloader)\n",
    "progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n",
    "losses_train = []\n",
    "for _ in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    loss, _ = forward(data, model, device, criterion)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses_train.append(loss.item())\n",
    "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "we can now run inference and store predicted and annotated trajectories. \n",
    "\n",
    "In this example, we run it on a single scene from the eval dataset because of computational constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running EVAL, loss: 564.3052978515625 loss(avg): 592.177718389602: 100%|██████████| 21/21 [00:39<00:00,  1.88s/it] \n"
     ]
    }
   ],
   "source": [
    "# ==== EVAL LOOP\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "losses_eval = []\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "future_coords_offsets_gt = []\n",
    "\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "progress_bar = tqdm(eval_dataloader)\n",
    "for data in progress_bar:\n",
    "    loss, ouputs = forward(data, model, device, criterion)\n",
    "    losses_eval.append(loss.item())\n",
    "    progress_bar.set_description(f\"Running EVAL, loss: {loss.item()} loss(avg): {np.mean(losses_eval)}\")\n",
    "\n",
    "    future_coords_offsets_pd.append(ouputs.reshape(len(ouputs), -1, 2).cpu().numpy())\n",
    "    future_coords_offsets_gt.append(data[\"target_positions\"].reshape(len(ouputs), -1, 2).cpu().numpy())\n",
    "\n",
    "    timestamps.append(data[\"timestamp\"].numpy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results  and perform evaluation\n",
    "After the model has predicted trajectories for our evaluation set, we can save them in a `csv` file. To simulate a complete evaluation session we can also save the GT in another `csv` and get the score.\n",
    "\n",
    "We will get `future_num_frames` values, corrisponding to the MSE (mean of squared errors) for that timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "| future step |   MSE   |\n",
      "+-------------+---------+\n",
      "|      1      |   1.25  |\n",
      "|      2      |   1.28  |\n",
      "|      3      |   2.62  |\n",
      "|      4      |   4.59  |\n",
      "|      5      |   3.60  |\n",
      "|      6      |   5.33  |\n",
      "|      7      |   7.43  |\n",
      "|      8      |  14.19  |\n",
      "|      9      |  14.58  |\n",
      "|      10     |  27.37  |\n",
      "|      11     |  37.77  |\n",
      "|      12     |  36.71  |\n",
      "|      13     |  63.62  |\n",
      "|      14     |  82.50  |\n",
      "|      15     |  104.25 |\n",
      "|      16     |  101.17 |\n",
      "|      17     |  117.21 |\n",
      "|      18     |  149.25 |\n",
      "|      19     |  174.30 |\n",
      "|      20     |  222.10 |\n",
      "|      21     |  229.88 |\n",
      "|      22     |  271.15 |\n",
      "|      23     |  288.86 |\n",
      "|      24     |  333.80 |\n",
      "|      25     |  344.32 |\n",
      "|      26     |  410.85 |\n",
      "|      27     |  458.43 |\n",
      "|      28     |  482.93 |\n",
      "|      29     |  520.63 |\n",
      "|      30     |  566.64 |\n",
      "|      31     |  639.03 |\n",
      "|      32     |  615.55 |\n",
      "|      33     |  754.85 |\n",
      "|      34     |  741.05 |\n",
      "|      35     |  850.06 |\n",
      "|      36     |  879.28 |\n",
      "|      37     |  991.11 |\n",
      "|      38     |  971.43 |\n",
      "|      39     | 1177.71 |\n",
      "|      40     | 1153.02 |\n",
      "|      41     | 1265.54 |\n",
      "|      42     | 1277.69 |\n",
      "|      43     | 1372.18 |\n",
      "|      44     | 1486.38 |\n",
      "|      45     | 1501.46 |\n",
      "|      46     | 1672.52 |\n",
      "|      47     | 1629.71 |\n",
      "|      48     | 1765.56 |\n",
      "|      49     | 1886.30 |\n",
      "|      50     | 1916.64 |\n",
      "+-------------+---------+\n"
     ]
    }
   ],
   "source": [
    "# ==== COMPUTE CSV\n",
    "pred_path = f\"{gettempdir()}/pred.csv\"\n",
    "gt_path = f\"{gettempdir()}/gt.csv\"\n",
    "\n",
    "write_coords_as_csv(pred_path, future_num_frames=cfg[\"model_params\"][\"future_num_frames\"],\n",
    "                    future_coords_offsets=np.concatenate(future_coords_offsets_pd),\n",
    "                    timestamps=np.concatenate(timestamps),\n",
    "                    agent_ids=np.concatenate(agent_ids))\n",
    "write_coords_as_csv(gt_path, future_num_frames=cfg[\"model_params\"][\"future_num_frames\"],\n",
    "                    future_coords_offsets=np.concatenate(future_coords_offsets_gt),\n",
    "                    timestamps=np.concatenate(timestamps),\n",
    "                    agent_ids=np.concatenate(agent_ids))\n",
    "# get a pretty visualisation of the errors\n",
    "table = PrettyTable(field_names=[\"future step\", \"MSE\"])\n",
    "table.float_format = \".2\"\n",
    "steps = range(1, cfg[\"model_params\"][\"future_num_frames\"] + 1)\n",
    "errors = compute_mse_error_csv(gt_path, pred_path)\n",
    "for step_idx, step_mse in zip(steps, errors):\n",
    "    table.add_row([step_idx, step_mse])\n",
    "print(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Results\n",
    "We can also visualise some results from the ego(AV) point of view. Let's have a look at the frame number `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent_dataset = eval_dataloader.dataset.datasets[0].dataset\n",
    "eval_ego_dataset = EgoDataset(cfg, eval_agent_dataset.dataset, rasterizer)\n",
    "frame_number = 0\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# get AV point-of-view frame\n",
    "data_ego = eval_ego_dataset[frame_number]\n",
    "im_ego = rasterizer.to_rgb(data_ego[\"image\"].transpose(1, 2, 0))\n",
    "\n",
    "\n",
    "center = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "agent_indices = eval_agent_dataset.get_frame_indices(frame_number)\n",
    "predicted_positions = []\n",
    "target_positions = []\n",
    "\n",
    "for v_index in agent_indices:\n",
    "    data_agent = eval_agent_dataset[v_index]\n",
    "\n",
    "    out_net = model(torch.from_numpy(data_agent[\"image\"]).unsqueeze(0).to(device))\n",
    "    out_pos = out_net[0].reshape(-1, 2).detach().cpu().numpy()\n",
    "    \n",
    "    # store absolute world coordinates\n",
    "    predicted_positions.append(out_pos + data_agent[\"centroid\"][:2])\n",
    "    target_positions.append(data_agent[\"target_positions\"] + data_agent[\"centroid\"][:2])\n",
    "\n",
    "\n",
    "# convert coordinates to AV point-of-view so we can draw them\n",
    "predicted_positions = transform_points(np.concatenate(predicted_positions), data_ego[\"world_to_image\"])\n",
    "target_positions = transform_points(np.concatenate(target_positions), data_ego[\"world_to_image\"])\n",
    "\n",
    "yaws = np.zeros((len(predicted_positions), 1))\n",
    "draw_trajectory(im_ego, predicted_positions, yaws, PREDICTED_POINTS_COLOR)\n",
    "draw_trajectory(im_ego, target_positions, yaws, TARGET_POINTS_COLOR)\n",
    "\n",
    "plt.imshow(im_ego[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
